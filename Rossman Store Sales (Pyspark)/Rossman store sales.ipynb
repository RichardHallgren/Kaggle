{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "from project_lib import Project\n",
    "project = Project(spark.sparkContext, '43285b0e-9e6e-4aad-8a96-3c40cf6f8874', 'p-feff31088fbf9bf46cb5e9470d87ed7c2432902c')\n",
    "pc = project.project_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': '",
    "    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token',\n",
    "    'api_key': '",
    "}\n",
    "\n",
    "configuration_name = 'os_c63666ea1c9d403e811d4c86742ad623_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_data_1 = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('train.csv', 'm5forecastingaccuracy-donotdelete-pr-5fybfotj7xtmkk'), inferSchema= 'true')\n",
    "#df_data_1.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Store=1, StoreType='c', Assortment='a', CompetitionDistance=1270, CompetitionOpenSinceMonth=9, CompetitionOpenSinceYear=2008, Promo2=0, Promo2SinceWeek=None, Promo2SinceYear=None, PromoInterval=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_store = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('store.csv', 'm5forecastingaccuracy-donotdelete-pr-5fybfotj7xtmkk'), inferSchema= 'true')\n",
    "df_store.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Sales\n",
       "0   1      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_c63666ea1c9d403e811d4c86742ad623 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='-ynWC7Y9F5nKlpdFO21VLMfHYNAPb03HUbUqKxnp07WH',\n",
    "    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "body = client_c63666ea1c9d403e811d4c86742ad623.get_object(Bucket='m5forecastingaccuracy-donotdelete-pr-5fybfotj7xtmkk',Key='sample_submission.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "sample_sub = pd.read_csv(body)\n",
    "sample_sub.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=1, Store=1, DayOfWeek=4, Date=datetime.datetime(2015, 9, 17, 0, 0), Open=1, Promo=1, StateHoliday='0', SchoolHoliday=0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_test = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(cos.url('test.csv', 'm5forecastingaccuracy-donotdelete-pr-5fybfotj7xtmkk'), inferSchema= 'true')\n",
    "df_test.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- StoreType: string (nullable = true)\n",
      " |-- Assortment: string (nullable = true)\n",
      " |-- CompetitionDistance: integer (nullable = true)\n",
      " |-- CompetitionOpenSinceMonth: integer (nullable = true)\n",
      " |-- CompetitionOpenSinceYear: integer (nullable = true)\n",
      " |-- Promo2: integer (nullable = true)\n",
      " |-- Promo2SinceWeek: integer (nullable = true)\n",
      " |-- Promo2SinceYear: integer (nullable = true)\n",
      " |-- PromoInterval: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_store.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in df_j: 1017209\n",
      "Number of records in df_j_test: 41088\n"
     ]
    }
   ],
   "source": [
    "#Join the data from df_store with the sales data in df_data_1\n",
    "\n",
    "df_j = df_data_1.join(df_store, \"Store\")\n",
    "df_j_test = df_test.join(df_store, \"Store\")\n",
    "\n",
    "print('Number of records in df_j:', df_j.count())\n",
    "print('Number of records in df_j_test:', df_j_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Sales: integer (nullable = true)\n",
      " |-- Customers: integer (nullable = true)\n",
      " |-- Open: integer (nullable = true)\n",
      " |-- Promo: integer (nullable = true)\n",
      " |-- StateHoliday: string (nullable = true)\n",
      " |-- SchoolHoliday: integer (nullable = true)\n",
      " |-- StoreType: string (nullable = true)\n",
      " |-- Assortment: string (nullable = true)\n",
      " |-- CompetitionDistance: integer (nullable = true)\n",
      " |-- CompetitionOpenSinceMonth: integer (nullable = true)\n",
      " |-- CompetitionOpenSinceYear: integer (nullable = true)\n",
      " |-- Promo2: integer (nullable = true)\n",
      " |-- Promo2SinceWeek: integer (nullable = true)\n",
      " |-- Promo2SinceYear: integer (nullable = true)\n",
      " |-- PromoInterval: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_j.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: integer (nullable = true)\n",
      " |-- Promo: integer (nullable = true)\n",
      " |-- StateHoliday: string (nullable = true)\n",
      " |-- SchoolHoliday: integer (nullable = true)\n",
      " |-- StoreType: string (nullable = true)\n",
      " |-- Assortment: string (nullable = true)\n",
      " |-- CompetitionDistance: integer (nullable = true)\n",
      " |-- CompetitionOpenSinceMonth: integer (nullable = true)\n",
      " |-- CompetitionOpenSinceYear: integer (nullable = true)\n",
      " |-- Promo2: integer (nullable = true)\n",
      " |-- Promo2SinceWeek: integer (nullable = true)\n",
      " |-- Promo2SinceYear: integer (nullable = true)\n",
      " |-- PromoInterval: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_j_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Store=1, DayOfWeek=5, Date=datetime.datetime(2015, 7, 31, 0, 0), Sales=5263, Customers=555, Open=1, Promo=1, StateHoliday='0', SchoolHoliday=1, StoreType='c', Assortment='a', CompetitionDistance=1270, Promo2=0, Promo2SinceWeek=None, Promo2SinceYear=None, PromoInterval=None, DaysSinceCompOpen=2524)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate days since competition opened\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "cols_to_drop = ['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'CompOpenSince']\n",
    "\n",
    "df_j = df_j.withColumn(\"CompOpenSince\", F.to_date(F.concat_ws(\"-\", \"CompetitionOpenSinceYear\", \"CompetitionOpenSinceMonth\")))\n",
    "df_j_test = df_j_test.withColumn(\"CompOpenSince\", F.to_date(F.concat_ws(\"-\", \"CompetitionOpenSinceYear\", \"CompetitionOpenSinceMonth\")))\n",
    "\n",
    "df_j = df_j.withColumn('DaysSinceCompOpen', F.datediff(df_j.Date, df_j.CompOpenSince))\n",
    "df_j_test = df_j_test.withColumn('DaysSinceCompOpen', F.datediff(df_j_test.Date, df_j_test.CompOpenSince))\n",
    "\n",
    "df_j = df_j.drop(*cols_to_drop)\n",
    "df_j_test = df_j_test.drop(*cols_to_drop)\n",
    "\n",
    "df_j.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month and week from date\n",
    "\n",
    "df_j = df_j.withColumn('Year',F.year('Date'))\n",
    "df_j = df_j.withColumn('Month',F.month('Date'))\n",
    "df_j = df_j.withColumn('Week',F.weekofyear('Date'))\n",
    "\n",
    "df_j_test = df_j_test.withColumn('Year',F.year('Date'))\n",
    "df_j_test = df_j_test.withColumn('Month',F.month('Date'))\n",
    "df_j_test = df_j_test.withColumn('Week',F.weekofyear('Date'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Sales: integer (nullable = true)\n",
      " |-- Customers: integer (nullable = true)\n",
      " |-- Open: integer (nullable = true)\n",
      " |-- Promo: integer (nullable = true)\n",
      " |-- StateHoliday: string (nullable = true)\n",
      " |-- SchoolHoliday: integer (nullable = true)\n",
      " |-- StoreType: string (nullable = true)\n",
      " |-- Assortment: string (nullable = true)\n",
      " |-- CompetitionDistance: integer (nullable = true)\n",
      " |-- Promo2: integer (nullable = true)\n",
      " |-- Promo2SinceWeek: integer (nullable = true)\n",
      " |-- Promo2SinceYear: integer (nullable = true)\n",
      " |-- PromoInterval: string (nullable = true)\n",
      " |-- DaysSinceCompOpen: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Week: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_j.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change datatype of some columns with wrong dtype\n",
    "\n",
    "cols_to_double = ['DaysSinceCompOpen', 'CompetitionDistance']\n",
    "\n",
    "for col in cols_to_double:\n",
    "    df_j = df_j.withColumn(col, df_j[col].cast('double'))\n",
    "    df_j_test = df_j_test.withColumn(col, df_j_test[col].cast('double'))\n",
    "\n",
    "df_j = df_j.withColumn('Sales', df_j['Sales'].cast('double'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Promo2SinceWeek</th>\n",
       "      <td>508031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Promo2SinceYear</th>\n",
       "      <td>508031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DaysSinceCompOpen</th>\n",
       "      <td>323348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <td>2642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count\n",
       "Promo2SinceWeek      508031\n",
       "Promo2SinceYear      508031\n",
       "DaysSinceCompOpen    323348\n",
       "CompetitionDistance    2642"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_missings(spark_df,sort=True):\n",
    "    df = spark_df.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) for (c,c_type) in spark_df.dtypes if c_type not in ('timestamp', 'string', 'date')]).toPandas()\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"There are no any missing values!\")\n",
    "        return None\n",
    "\n",
    "    if sort:\n",
    "        df = df.rename(index={0: 'count'}).T.sort_values(\"count\",ascending=False)\n",
    "        return df[(df['count'] > 0)]\n",
    "\n",
    "    return df\n",
    "count_missings(df_j, sort=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CompetitionDistance: 75860.0\n"
     ]
    }
   ],
   "source": [
    "# Extract the maximum distance to a competitor store\n",
    "\n",
    "max_comp_distance = df_j.agg({'CompetitionDistance': 'max'}).collect()[0][0]\n",
    "print('Max CompetitionDistance:', max_comp_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values\n",
    "\n",
    "# For DaysSinceCompOpen fill in 0 as the missing value\n",
    "df_j = df_j.fillna(0, subset='DaysSinceCompOpen')\n",
    "df_j_test = df_j_test.fillna(0, subset='DaysSinceCompOpen')\n",
    "\n",
    "# For CompetitionDistance assume there is no competition store closer than the maximum competition distance + 1\n",
    "df_j = df_j.fillna(max_comp_distance+1, subset='CompetitionDistance')\n",
    "df_j_test = df_j_test.fillna(max_comp_distance+1, subset='CompetitionDistance')\n",
    "\n",
    "# Fill in nulls with 0\n",
    "cols_for_pred = ['PromoInterval', 'Assortment', 'StoreType', 'StateHoliday', 'Store', 'DayOfWeek', 'Open', 'Promo', 'SchoolHoliday', 'CompetitionDistance', 'Promo2', 'DaysSinceCompOpen']\n",
    "\n",
    "df_test = df_test.fillna(0, subset=cols_for_pred)\n",
    "df_j_test = df_j_test.fillna(0, subset=cols_for_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT DayOfWeek)|\n",
      "+-------------------------+\n",
      "|                        7|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_j.select(F.countDistinct(\"DayOfWeek\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a small sample of df_j to build up a model\n",
    "df_js = df_j.sample(False, 0.0001, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Store', 'DayOfWeek', 'Open', 'Promo', 'SchoolHoliday', 'CompetitionDistance', 'Promo2', 'DaysSinceCompOpen', 'Year', 'Month', 'Week', 'PromoInterval', 'Assortment', 'StoreType', 'StateHoliday', 'Sales']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "cat_cols = ['PromoInterval', 'Assortment', 'StoreType', 'StateHoliday']\n",
    "num_cols = ['Store', 'DayOfWeek', 'Open', 'Promo', 'SchoolHoliday', 'CompetitionDistance', 'Promo2', 'DaysSinceCompOpen', 'Year', 'Month', 'Week']\n",
    "\n",
    "cols_for_model = num_cols + cat_cols + ['Sales']\n",
    "\n",
    "df_j_red = df_j[cols_for_model]\n",
    "\n",
    "stages = []\n",
    "\n",
    "# Convert categorical strings to index values\n",
    "for cat_col in cat_cols:\n",
    "    indexer = StringIndexer(inputCol=cat_col, outputCol= cat_col +'_idx', handleInvalid='keep')\n",
    "    onehot = OneHotEncoderEstimator(inputCols=[indexer.getOutputCol()], outputCols= [cat_col + '_dummy'])\n",
    "    stages += [indexer, onehot]\n",
    "\n",
    "ohecols = [col + '_dummy' for col in cat_cols]\n",
    "    \n",
    "# Assemble predictors into a single column \n",
    "assembler = VectorAssembler(inputCols=num_cols + ohecols, outputCol='features')\n",
    "\n",
    "stages += [assembler]\n",
    "\n",
    "s_train, s_test = df_j_red.randomSplit([0.8, 0.2], seed=1)\n",
    "\n",
    "# Print out columns to be used in prediction\n",
    "print(s_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradient boosted tree regressor\n",
    "gbt = GBTRegressor(featuresCol='features',\n",
    "                           labelCol='Sales',\n",
    "                           seed=1,\n",
    "                           stepSize = 0.05,\n",
    "                           maxIter = 100,\n",
    "                           maxDepth = 4\n",
    "                           )\n",
    "\n",
    "\n",
    "stages += [gbt]\n",
    "\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Create the evaluator, evaluate on Root mean square error\n",
    "pip_evaluator = RegressionEvaluator(\n",
    "    labelCol='Sales', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexer_a7324f6e293b, OneHotEncoderEstimator_d27478aff88f, StringIndexer_4da27289da9a, OneHotEncoderEstimator_dd2d4d600659, StringIndexer_4bb8121ed221, OneHotEncoderEstimator_29f067d30cf3, StringIndexer_9beddf6a1ca9, OneHotEncoderEstimator_7a272c173783, VectorAssembler_bde647bbcc1d, GBTRegressionModel (uid=GBTRegressor_b2bdfa6d1497) with 100 trees]\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5, current: 4)\n",
      "Root Mean Squared Error (RMSE) on test data = 3017.22\n"
     ]
    }
   ],
   "source": [
    "def gbt_with_cross_validation():\n",
    "    # Create a parameter grid to try different parameters\n",
    "    paramGrid = (ParamGridBuilder()\n",
    "                 .addGrid(gbt.maxDepth, [4, 5, 6])\n",
    "                 .build())\n",
    "\n",
    "\n",
    "\n",
    "    # Use cross validation to find the best model\n",
    "    cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=pip_evaluator, numFolds=5, seed=1)\n",
    "\n",
    "    cvModel = cv.fit(s_train)\n",
    "\n",
    "    # Get the best model from cross validation\n",
    "    best_model = cvModel.bestModel\n",
    "\n",
    "    # Look at the stages in the best model\n",
    "    print(best_model.stages)\n",
    "\n",
    "    # select last stage (the GBTRegressor) of the best model and see info regarding which depth the model has chosen, this can be used on any parameter\n",
    "    print(best_model.stages[-1].explainParam('maxDepth'))\n",
    "\n",
    "    # Generate predictions on testing data using the best model then calculate RMSE\n",
    "    predictions = best_model.transform(s_test)\n",
    "    rmse = pip_evaluator.evaluate(predictions)\n",
    "\n",
    "    print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "    \n",
    "    return cvModel.bestModel\n",
    "\n",
    "model = gbt_with_cross_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbt_model():\n",
    "    g_model = pipeline.fit(s_train)\n",
    "    \n",
    "    predictions = g_model.transform(s_test)\n",
    "    rmse = pip_evaluator.evaluate(predictions)\n",
    "\n",
    "    print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "    \n",
    "    return g_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2150.58\n"
     ]
    }
   ],
   "source": [
    "model = gbt_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the columns\n",
    "df_j_test_red = df_j_test[num_cols + cat_cols]\n",
    "#df_test = df_test.fillna(0, subset=columns_num)\n",
    "\n",
    "# Make prediction\n",
    "kaggle_pred = model.transform(df_j_test_red)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_pandas = kaggle_pred.select('prediction').toPandas()\n",
    "ki_pandas = df_j_test.select('Id').toPandas()\n",
    "kp_pandas['Id'] = ki_pandas['Id']\n",
    "kp_pandas['Sales'] = kp_pandas['prediction']\n",
    "kp_pandas = kp_pandas.drop(columns=['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7185.401230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7056.834693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8653.083220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7362.337956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>8480.978733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id        Sales\n",
       "0   1  7185.401230\n",
       "1   2  7056.834693\n",
       "2   3  8653.083220\n",
       "3   4  7362.337956\n",
       "4   5  8480.978733"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kp_pandas.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'submission13.csv',\n",
       " 'message': 'File saved to project storage.',\n",
       " 'bucket_name': 'm5forecastingaccuracy-donotdelete-pr-5fybfotj7xtmkk',\n",
       " 'asset_id': 'ab60918a-f03c-40c2-b77c-6a3bd58b44f3'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.save_data(\"submission13.csv\", kp_pandas.to_csv(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
